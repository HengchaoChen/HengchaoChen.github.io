---
title: "Readings: Riemannian Geometry and Statistical Machine Learning"
subtitle: "Week 1"
author: "Presenter: Archer Hengchao Chen"
date: "2021/1/20"
output: 
  slidy_presentation:
    number_sections : FALSE
    incremental : FALSE
    slide_level : NULL
    duration : NULL
    footer : NULL
    font_adjustment : -1
    fig_width : 8
    fig_height : 6
    fig_retina : 2
    fig_caption : TRUE
    dev : "png"
    df_print : "default"
    self_contained : TRUE
    highlight : "default"
    mathjax : "default"
    template : "default"
    css : NULL
    includes : NULL
    keep_md : FALSE
    lib_dir : NULL
    md_extensions : NULL
    pandoc_args : NULL
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1 
There are two fundamental spaces in machine learning:

* data space $\mathcal{X}$
  + $\mathcal{X}$ in the generative case
  + $\mathcal{X}\times\mathcal{Y}$ in the discriminative case

> Note: in the supervised case, we focus on the classification setting, where $\mathcal{Y}=\{y_1,\cdots,y_k\}$ is a finite set. By this, we mean
\begin{align}
\mathcal{X}\times\mathcal{Y} = \mathcal{X}\times\cdots\times\mathcal{X} = \mathcal{X}^k
\end{align}
has the product geometry over $\mathcal{X}$. Therefore, this thesis only study the data spaces of $\mathcal{X}$.

* model space $\Theta$
  + $\{p(x;\theta):\theta\in\Theta\}$ in the generative case
  + $\{p(y|x;\theta):\theta\in\Theta\}$ in the discriminative case

**What are Learning algorithms?** Learning algorithms select a model $\theta\in\Theta$ based on a training sample $\{x_i\}_{i=1}^n\subset\mathcal{X}$ in the generative case or $\{(x_i,y_i)\}_{i=1}^n\subset\mathcal{X}\times\mathcal{Y}$ in the discriminative case.

## 2
**Data and model spaces are rarely Euclidean spaces.** (not vector spaces)

* data space $\mathcal{X}$:
  + the representation of text documents as vectors
  + images $I\in[a,b]^{k\times m}$
* model space $\Theta$:
  + normal, exponential, Dirichlet or multinomial, as well as the set of non-negative, normalized distribution

The Euclidean geometry is thus artificial on most data and model spaces. 

**Using Riemannian Geometry to model the geometries of $\mathcal{X}$ and $\Theta$.** 

Despite most data and model spaces are not Euclidean, they are **smooth** and **locally Euclidean**. Manifolds are the natural generalization of Euclidean spaces to locally Euclidean spaces, smooth manifolds are their smooth counterparts, and Riemannian manifolds are smooth manifolds with a geometric structure called the Riemannian metric. 

**Purposes:**  
**statistical learning algorithms --------- geometries of $\mathcal{X}$ and $\Theta$.**

## 3
Topics involved:

1. geometry of the spaces $\Theta$ of the conditional models underlying the algorithms logistic regression and AdaBoost
2. generalization of the theorems of Cencov and Campbell to the conditional models
3. an embedding principle to obtain a natural geometry on the data space $\mathcal{X}$
4. new algorithm: generalization of radial basis kernel by the diffusion kernels on manifolds
5. new algorithm: generalization of linear classifiers to non-Euclidean geometries, specifically, the multinomial analogue of logistic regression
6. metric learning based on a given data set

The first 5 parts are based on Fisher information geometry, the only natural geometry in the sense of the Cencov's theorem. The last part is an adaptive method. For a given data set $\{x_1,\cdots,x_n\}\subset\mathcal{X}$, it may be possible to induce a geometry that is better suited for it.

By studying geometry of data/model spaces, we may get insights of some algorithms and design new algorithms that outperform the existing ones.

## 4
**What is Fisher information geometry?** On a space of statistical models $\Theta\subset\mathbb{R}^n$, we set the **Fisher information metric** $g_{\theta}(u,v),\theta\in\Theta$ to be the Fisher information $\mathcal{J}_{\theta}(u,v)$
\begin{align}
\mathcal{J}_{\theta}(u,v)\overset{\text{def}}{=}\sum_{i=1}^n\sum_{j=1}^nu_iv_j\int p(x;\theta)\frac{\partial}{\partial\theta_i}\log p(x;\theta)\frac{\partial}{\partial\theta_j}\log p(x;\theta)dx,
\end{align}
where the above integral is replaced with a sum if $\mathcal{X}$ is discrete. Note: only defined for model spaces $\Theta$ when the data space $\mathcal{X}$ and the link function (p.d.f.) $p(x;\theta),x\in\mathcal{X}$ is known.

**Compare with Euclidean metric:** The Euclidean geometry on $\mathcal{X}=\mathbb{R}^n$ is achived by setting the local metric $g_x(u,v),x\in\mathcal{X}$ to be the Euclidean inner product
\begin{align}
g_x(u,v)\overset{\text{def}}{=}\langle u,v\rangle\overset{\text{def}}{=}\sum_{i=1}^nu_iv_i.
\end{align}

**Why is Fisher information geometry unique?**
The Cencov's theorem states that the Fisher information metric is the only Riemannian metric that is preserved under basic probabilistic transformations, called congruent embedding by a Markov morphism, which represent transformations of the event space that is equivalent to extracting a sufficient statistic.

## 5
[Introduction to Smooth Manifolds, John M.Lee](https://link.springer.com/book/10.1007/978-1-4419-9982-5) is a good introduction to the smooth manifold, while [Riemannian Geometry, do Carmo](https://link.springer.com/book/10.1007%2F978-3-642-18855-8) is a classical textbook in Riemannian geometry. 

**Definition (Topological Manifold).** We say a topological space $\mathcal{M}$ is a topological manifold of dimension $n$ if $\mathcal{M}$ is Hausdorff, second countable, and locally Euclidean of dimension $n$.

A **coordinate chart on $\mathcal{M}$** is a pair $(U,\varphi)$, where $U$ is an open subset of $\mathcal{M}$ and $\varphi:U\to\varphi(U)$ is a homeomorphism from $U$ to an open subset $\widetilde{U}=\varphi(U)\subset\mathbb{R}^n$. 

**Definition ($C^{\infty}$ compatible).** Two coordinate charts $(U,\varphi)$ and $(V,\psi)$ are called $C^{\infty}$ compatible if $U\cap V$ nonempty implies that $\psi\circ\varphi^{-1}$ is a diffeomorphism from $\varphi(U\cap V)$ to its image.

**Definition (Smooth Structure).** A smooth structure on a manifold $\mathcal{M}$ is a family $\mathcal{U}=(U_{\alpha},\varphi_{\alpha})$ of coordinate charts such that

- $U_{\alpha}$ cover $\mathcal{M}$
- $(U_{\alpha},\varphi_{\alpha})$ and $(U_{\beta},\varphi_{\beta})$ are $C^{\infty}$ compatible for any $\alpha$, $\beta$
- it is maximal, i.e. any coordinate chart $(V,\psi)$ that is $C^{\infty}$ compatible with all $(U_{\alpha},\phi_{\alpha})$ is in $\mathcal{U}$

## 6
A **smooth manifold $\mathcal{M}$** is a topological manifold with a smooth structure. 

Examples of smooth manifolds include:

- Euclidean spaces, smooth curves and surfaces in Euclidean spaces. 
- n-sphere $\mathbb{S}^n\overset{\text{def}}{=}\{x\in\mathbb{R}^{n+1}:\sum_{i=1}^{n+1}x_i^2=1\}$.
- positive orthant of the n-sphere $\mathbb{S}^n_+\overset{\text{def}}{=}\{x\in\mathbb{R}^{n+1}:\sum_{i=1}^{n+1}x_i^2=1,\ x_i\geq0\}$.
- n-simplex $\mathbb{P}_n\overset{\text{def}}{=}\{x\in\mathbb{R}^{n+1}:\sum_{i=1}^{n+1}x_i=1,\ x_i>0\}$.

A **smooth manifold with boundary** is defined similarly in [Introduction to Smooth Manifolds, John M.Lee](https://link.springer.com/book/10.1007/978-1-4419-9982-5). 

Examples include the upper half plane $\mathbb{H}^n\overset{\text{def}}{=}\{x\in\mathbb{R}^n:x_n\geq 0\}$.

## 7

**Definition (Smooth functions).** A continuous function $f:\mathcal{M}\to\mathbb{R}$ is said to be $C^{\infty}$ differentiable if for every chart $(U,\varphi)$ the function $f\circ\varphi^{-1}\in C^{\infty}(\mathbb{R}^n,\mathbb{R})$. We use $C^{\infty}(\mathcal{M})$ to denote the sets of all smooth functions on $\mathcal{M}$.

**Definition (Smooth mappings).** A continuous mapping between two differentiable manifolds $f:\mathcal{M}\to \mathcal{N}$ is said to be $C^{\infty}$ differentiable if $\forall r\in C^{\infty}(\mathcal{N},\mathbb{R}),\ r\circ f\in C^{\infty}(\mathcal{M},\mathbb{R})$. We use $C^{\infty}(\mathcal{M},\mathcal{N})$ to denote the sets of all smooth mappings from $\mathcal{M}$ to $\mathcal{N}$.

A **diffeomorphism** between two maifolds $\mathcal{M},\mathcal{N}$ is a bijection $f:\mathcal{M}\to \mathcal{N}$ such that $f,f^{-1}\in C^{\infty}(\mathcal{M})$.

**Definition (Tangent Bundles).** Let $p$ be a point on a manifold $\mathcal{M}$. A tangent vector at $p$ is a linear map $X_p:C^{\infty}(\mathcal{M})\to\mathbb{R}$ satisfying the product rule, i.e. 
\begin{align}
X_p(fg)=X_p(f)g(p)+f(p)X_p(g),
\end{align}
for any $f,g\in C^{\infty}(\mathcal{M})$. The tangent space to $\mathcal{M}$ at $p$ is the collection of all tangent vectors at $p$, denoted by $T_p\mathcal{M}$. The tangent bundle of $\mathcal{M}$, denoted by $T\mathcal{M}$, is the disjoint union of the tangent spaces at all points of $\mathcal{M}$:
\begin{align}
T\mathcal{M}=\prod_{p\in \mathcal{M}}T_p\mathcal{M}.
\end{align}
One can show that $T\mathcal{M}$ has a natural structure as a smooth manifold in its own right.

## 8
**Definition (Push-forward).** If $\mathcal{M}$ and $\mathcal{N}$ are smooth manifolds and $F:\mathcal{M}\to \mathcal{N}$ is a smooth map, then for each $p$ we define a map $F_*:T_p\mathcal{M}\to T_{F(p)}\mathcal{N}$, called the push-forward associated with $F$, by 
\begin{align}
(F_*X)(f)=X(f\circ F).
\end{align}

**Proposition (Local representation of the tangent space).** For each point $p$ on a smooth manifold $\mathcal{M}$, the tangent space $T_p\mathcal{M}$ is a vector space of dimension $n$. Given a coordinate chart $(U,\varphi)$ around $p$, we can naturally define a set of basis $\left\{\partial_i,i=1,\cdots,n\right\}$ of the tangent space $T_p\mathcal{M}$ by
\begin{align}
\partial_i=(\varphi^{-1})_*\frac{\partial}{\partial x^i}\large|_{\varphi(p)}.
\end{align}

**Definition (Tangent Vector Field).** A smooth tangent vector field $X$ on $\mathcal{M}$ is a smooth assignment of tangent vectors to each point of $\mathcal{M}$. More precisely, a vector field $X$ maps each point $p$ to a tangent vector $X_p\in T_p\mathcal{M}$. Then for any function $f\in C^{\infty}(\mathcal{M})$ we can define the action of $X$ on $f$ as 
\begin{align}
Xf:\mathcal{M}\to\mathbb{R},\ (Xf)(p)=X_p(f).
\end{align}
The smoothness in the definition is in the sense that $\forall f\in C^{\infty}(\mathcal{M}),Xf\in C^{\infty}(\mathcal{M})$.

## 9
In many cases the $n$-manifold $\mathcal{M}$ is a submanifold of a $m$-dimenisonal manifold $\mathcal{N}$, $m\geq n$. Specifically, when $\mathcal{N}=\mathbb{R}^m$, the tangent space $T_x\mathcal{M}$ of the submanifold is a vector subspace of $T_x\mathbb{R}^m\cong\mathbb{R}^m$ and we may represent a tangent vector $v\in T_x\mathcal{M}$ in the standard basis $\{\partial_i\}_{i=1}^m$ of the embedding tangent space $T_x\mathbb{R}^m$ as $v=\sum_{i=1}^mv_i\partial_i$. For example, for the simplex and the sphere we have
\begin{align}
T_x\mathbb{P}_n=\left\{v\in\mathbb{R}^{n+1}:\sum_{i=1}^{n+1}v_i=0\right\},\qquad T_x\mathbb{S}^n=\left\{v\in\mathbb{R}^{n+1}:\sum_{i=1}^{n+1}v_ix_i=0\right\}.
\end{align}

<center><img src="C:/Users/asus-pc/Desktop/2021 S/Rie Geo & Stat ML/Pre & Slides/images/tangent_vector.png" width=50% height=50%/></center>

## 10
**Definition (Riemannian Manifold).** A Riemannian manifold $(\mathcal{M},g)$ is a smooth manifold $\mathcal{M}$ equipped with a Riemannian metric $g$. The metric $g$ is defined by a local inner product on tangent vectors
\begin{align}
g_x(\cdot,\cdot):T_x\mathcal{M}\times T_x\mathcal{M}\to\mathbb{R},\ x\in \mathcal{M},
\end{align}
that is symmetric, bi-linear, and positive definite, and is also smooth in $x$.

One can define the **length of a smooth curve** $\gamma:[a,b]\to \mathcal{M}$ by 
\begin{align}
L(\gamma)=\int_a^b\sqrt{g_x(\dot{\gamma}(t),\dot{\gamma}(t))}dt,
\end{align}
where $\dot{\gamma}(t)=\gamma_*(\frac{d}{dt})$ is the velocity vector of the curve $\gamma$ at time $t$. Then one can define the distance $d_g(x,y)$ between two points $x,y\in \mathcal{M}$ as 
\begin{align}
d_g(x,y)=\inf_{\gamma\in\Gamma(x,y)}\int_a^b\sqrt{g_x(\dot{\gamma}(t),\dot{\gamma}(t))}dt,
\end{align}
where $\Gamma(x,y)$ is the set of piecewise smooth curves connecting $x$ and $y$. The distance is called **geodesic distance** and the minimal curve achieving it is called a **geodesic curve**.

## 11
**Definition (Pull-back metric).** Given a Riemannian manifold $(\mathcal{N},h)$ and a diffeomorphism $f:\mathcal{M}\to\mathcal{N}$, we define a metric $f^*h$ on $\mathcal{M}$ called the pull-back metric by 
\begin{align}
(f^* h)_x(u,v)=h_{f(x)}(f_* u,f_* v).
\end{align}

**Definition (Isometry).** An isomorphism is a diffeomorphism $f:\mathcal{M}\to \mathcal{N}$ between two Riemannian manifolds $(\mathcal{M},g),(\mathcal{N},h)$ for which 
\begin{align}
g_x(u,v)=(f^*h)_x(u,v),\ \forall x\in \mathcal{M},\ \forall u,v\in T_x\mathcal{M}.
\end{align} 
Two Riemannian manifolds are called isometric if there exist an isomorphism between them.

## 12
**Fisher geometries of spaces of probability models** --- A few important examples:

1. finite non-parametric space
2. finite conditional non-parametric space
3. spherical normal space

**Finite non-parametric space / parametric space of the multinomial family.** In finite non-parametric setting, the event space $\mathcal{X}$ is a finite set with $|\mathcal{X}|=n$ and $\Theta=\mathbb{P}_{n-1}$, which represents the manifold of all positive probability models over $\mathcal{X}$. Note that the event space always enjoys a representation as $\mathcal{X}=\left\{x\in\{0,1\}^n:\sum_ix_i=1\right\}$. If zero probabilities are admitted, the appropriate framework for the parameter space $\Theta=\overline{\mathbb{P}_{n-1}}$ is a manifold with corners (Lee, 2002). Note that the above space $\Theta$ is precisely the parametric space of the multinomial distribution.

## 13
As mentioned earlier, the tangent vector $v\in T_{\theta}\mathbb{P}_{n-1}$ has the form $v=\sum_{i=1}^nv_i\partial_i$, subject to $\sum_{i=1}^nv_i=0$. Using this representation, the loglikelihood and its derivatives are
\begin{align}
\log p(x;\theta)&=\sum_{i=1}^nx_i\log \theta_i,\\
\frac{\partial\log p(x;\theta)}{\partial\theta_i}&=\frac{x_i}{\theta_i},\\
\frac{\partial^2\log(p(x;\theta))}{\partial\theta_i\partial\theta_j}&=-\frac{x_i}{\theta_i^2}\delta_{ij},
\end{align}
where $x\in\mathcal{X}=\left\{x\in\{0,1\}^n:\sum_ix_i=1\right\}$. Then we can calculate the **Fisher information metric on $\mathbb{P}_{n-1}$** as
\begin{align}
\mathcal{J}_{\theta}(u,v)=-\sum_{i=1}^n\sum_{j=1}^nu_iv_j\mathbb{E}\left[\frac{\partial^2\log(p(x;\theta))}{\partial\theta_i\partial\theta_j}\right]=-\sum_{i=1}^n\mathbb{E}[-x_i/\theta_i^2]=\sum_{i=1}^n\frac{u_iv_i}{\theta_i},
\end{align}
since $\mathbb{E}x_i=\theta_i$.

## 14
To calculate the geodesic distances on $\mathbb{P}_{n-1}$, one simple method is based on the isomorphism from $\mathbb{P}_{n-1}$ to the positive orthant $\tilde{\mathbb{S}}^{n-1}_+ =\left\{\theta\in\mathbb{R}^n:\sum_{i=1}^n\theta_i^2=4,\theta_i>0\right\}$ of the n-sphere of radius 2. More precisely, the **isomorphism**, its **inverse**, and its **inverse's push-forward** is given by
\begin{align}
&F:\mathbb{P}_{n-1}\to\tilde{\mathbb{S}}^{n-1}_+,\ F(\theta_1,\cdots,\theta_n)=(2\sqrt{\theta_1},\cdots,2\sqrt{\theta_n}),\\
&F^{-1}:\tilde{\mathbb{S}}^{n-1}_+\to\mathbb{P}_{n-1},\ F^{-1}(\theta_1,\cdots,\theta_n)=(\frac{\theta_1^2}{4},\cdots,\frac{\theta_n^2}{4}),\\
&F_*^{-1}\large|_{\theta}:T_{\theta}\tilde{\mathbb{S}}^{n-1}_+\to T_{F^{-1}(\theta)}\mathbb{P}_{n-1},\ F_*^{-1}(u_1,\cdots,u_n)=(\frac{u_1\theta_1}{2},\cdots,\frac{u_n\theta_n}{2}),
\end{align}
where $u=(u_1,\cdots,u_n)$ stands for coordinate representation of a tangent vector in the standard basis of $T_{\theta}\mathbb{R}^n$. The push-forward calculation is as follows:
\begin{align}
(F^{-1}_*(\sum_{i=1}^n u_ie_i))f=\sum_{i=1}^n u_ie_i(f\circ F^{-1})\overset{\text{chain rule}}{=}\sum_{i=1}^n u_i(\sum_{j=1}^ne_j(f)\frac{\partial F^{-1}_j}{\partial \theta_i})=\sum_{i=1}^n\frac{u_i\theta_i}{2}e_i(f).
\end{align}

## 15
The **pull-back metric on $\tilde{\mathbb{S}}^{n-1}_+$ through $F^{-1}$** is
\begin{align}
h_\theta(u,v)=\mathcal{J}_{\theta^2/4}(F_*^{-1}u,F_*^{-1}v)=\mathcal{J}_{\theta^2/4}(\frac{u\theta}{2},\frac{v\theta}{2})\\
=\sum_i\frac{u_i\theta_i/2*v_i\theta_i/2}{\theta_i^2/4}=\sum_iu_iv_i=\delta_{\theta}(u,v),
\end{align}
where $\delta_{\theta}(u,v)$ is the Euclidean metric on $\tilde{\mathbb{S}}^{n-1}_+$ inherited from the embedding space $\mathbb{R}^n$. This proves the **isometric** property.

Then the **geodesic distance $d_{\mathcal{J}}(\theta,\theta')$ on $\mathbb{P}_{n-1}$** may be computed as the shortest curve on $\tilde{\mathbb{S}}^n_+$ connecting $F(\theta)$ and $F(\theta')$. These shortest curves are portions of great circles and the distance is 
\begin{align}
d_{\mathcal{J}}(\theta,\theta')=d_{\delta}(F(\theta),F(\theta'))=2\arccos\left(\sum_{i=1}^n\sqrt{\theta_i\theta_i'}\right).
\end{align}

- The geodesic distance $d_{\mathcal{J}}(\theta,\theta')$ and the K-L divergence $D(\theta,\theta')$ agree up to second order as $\theta\to\theta'$. 
- The Hellinger distance $d_H(\theta,\theta')=\sqrt{\sum_i(\sqrt{\theta_i}-\sqrt{\theta_i'})^2}$ also agrees with $d_{\mathcal{J}}(\theta,\theta')$ up to second order as $\theta\to\theta'$.

## 16
**Finite non-parametric conditional models.** Given two finite sets $\mathcal{X},\mathcal{Y}$ of sizes $k$ and $m$ respectively, a conditional probability model $p(y|x)$ reduces to an element of $\mathbb{P}_{m-1}$ for each $x\in\mathcal{X}$. We may thus identify the space of conditional probability models associated with $\mathcal{X}$ and $\mathcal{Y}$ as the product space $\mathbb{P}_{m-1}^k=\mathbb{P}_{m-1}\times\cdots\times\mathbb{P}_{m-1}$.

**Finite non-parametric conditional models. (non-normalized)** Dropping the normailzation constraints $\sum_ip(y_i|x_j)=1$ we obtain non-normalized conditional models in the cone of $k\times m$ matrices with positive entries, denoted by $\mathbb{R}_+^{k\times m}$.

A conditional model (normalized or not) is described by a positive matrix $M$ such that $M_{ij}=p(y_j|x_i)$. They are (row) stochastic matrices. We denote the **tangent spaces to $\mathbb{R}^{k\times m}_+$** using the standard basis
\begin{align}
T_M\mathbb{R}^{k\times m}_+=\text{span}\{\partial_{ij}:i=1,\cdots,k,j=1,\cdots,m\}.
\end{align}
The **tangent vectors to $\mathbb{P}_{m-1}^k$** are linear combinations of $\{\partial_{ij}\}$ such that the sums of the combination coefficients over each row are 0, e.g.
\begin{align}
\frac{1}{2}\partial_{11}+\frac{1}{2}\partial_{12}-\partial_{13}+\frac{1}{3}\partial_{21}-\frac{1}{3}\partial_{22}&\in T_M\mathbb{P}^3_2,\\
\frac{1}{2}\partial_{11}+\frac{1}{2}\partial_{12}-\partial_{13}+\frac{1}{3}\partial_{21}-\frac{1}{2}\partial_{22}&\notin T_M\mathbb{P}^3_2.
\end{align}

## 17
Both of the conditional models are product manifolds, so it is natural to set the Riemannian metric as the **product Fisher information metric on $\mathbb{P}^k_{n-1}$ and $\mathbb{R}^{k\times m}_+$**. Using the above representation, for tangent vectors $u,v\in T_M\mathbb{R}^{k\times m}_+$ or $u,v\in T_M\mathbb{P}^k_{m-1}$, the product Fisher information is
\begin{align}
\mathcal{J}_M^k(u_1\oplus\cdots\oplus u_k,v_1\oplus\cdots\oplus v_k)&\overset{\text{def}}{=}(\mathcal{J}\otimes\cdots\otimes\mathcal{J})_M(u_1\oplus\cdots\oplus u_k,v_1\oplus\cdots\oplus v_k)\\
&\overset{\text{def}}{=}\sum_{i=1}^k\mathcal{J}_{M_{i\cdot}}(u_i,v_i),
\end{align}
where $M_{i\cdot}$ is the $i$-row of the matrix $M$. This reduces to
\begin{align}
\mathcal{J}_M^k(u,v)=\sum_{i=1}^k\sum_{j=1}^m\frac{u_{ij}v_{ij}}{M_{ij}}.
\end{align}

## 18
**Spherical normal spaces.** Now the event space is $\mathcal{X}=\mathcal{R}^{n-1}$, the model is $\left\{\rm{N}(\mu,\sigma I):\mu\in\mathbb{R}^{n-1},\sigma\in\mathbb{R}_+\right\}$, and the model space is $\Theta=\mathbb{H}^n\cong\mathbb{R}^{n-1}\times\mathbb{R}_+$. The **Fisher information metric** is given by the corresponding Gram matrix $G(\theta)$. Simple calculations yield, for $1\leq i,j\leq n-1$,
\begin{align}
[G(\theta)]_{ij}&=-\int_{\mathbb{R^{n-1}}}\frac{\partial^2}{\partial \mu_i\mu_j}\left(-\sum_{k=1}^{n-1}\frac{(x_k-\mu_k)^2}{2\sigma^2}\right)p(x|\theta)dx=\frac{1}{\sigma^2}\delta_{ij},\\
[G(\theta)]_{ni}&=-\int_{\mathbb{R^{n-1}}}\frac{\partial^2}{\partial \sigma\mu_j}\left(-\sum_{k=1}^{n-1}\frac{(x_k-\mu_k)^2}{2\sigma^2}\right)p(x|\theta)dx=\frac{2}{\sigma^3}\int_{\mathbb{R}^{n-1}}(x_i-\mu_i)p(x|\theta)dx=0,\\
[G(\theta)]_{nn}&=-\int_{\mathbb{R^{n-1}}}\frac{\partial^2}{\partial \sigma\partial\sigma}\left(-\sum_{k=1}^{n-1}\frac{(x_k-\mu_k)^2}{2\sigma^2}-(n-1)\log\sigma\right)p(x|\theta)dx\\
&=\frac{3}{\sigma^4}\int_{\mathbb{R}^{n-1}}\sum_{k=1}^{n-1}(x_k-\mu_k)^2p(x|\theta)dx-\frac{n-1}{\sigma^2}=\frac{2(n-1)}{\sigma^2}.
\end{align}

Letting $\theta'$ be new coordinates defined by $\theta_i'=\mu_i$ for $1\leq i,j\leq n-1$ and $\theta_n'=\sqrt{2(n-1)}\sigma$, we see that the Gram matrix is given by $[G(\theta')]_{ij}=\frac{1}{\sigma^2}\delta_{ij}$.

## 19
To compute the geodesic distancce on $\mathbb{H}^n$, we transform points in $\mathbb{H}^n$ to an isometric manifold known as Poincare's ball. We first define the sphere invesion of $x$ with respect to a sphere $S$ with center $a$ and radius $r$ as
\begin{align}
I_S(x)=\frac{r^2}{\|x-a\|^2}(x-a)+a.
\end{align}
The Cayley's transform is the sphere inversion with respect to a sphere with center $(0,\cdots,0,-1)$ and radius $\sqrt{2}$. We denote by $\eta$ the inverse of the Cayley's transform that maps the hyperbolic half plane to Poincare's ball. In fact, 
\begin{align}
\eta(x)=-I_{S'}(-x),x\in\mathbb{H}^n,
\end{align}
where $S'$ is a sphere with center at $(0,\cdots,0,1)$ and radius $\sqrt{2}$. The **geodesic distance in $\mathbb{H}^n$** is then given by 
\begin{align}
d(x,y)=\text{acosh}\left(1+2\frac{\|\eta(x)-\eta(y)\|^2}{(1-\|\eta(x)\|^2)(1-\|\eta(y)\|^2)}\right),\ x,y\in\mathbb{H}^n.
\end{align}



